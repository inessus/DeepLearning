# 第六章　深度前馈网络（Deep Feedforward Networks）
  深度前馈网络，又称*前馈神经网络*，多层感知机，是典型的深度学习模型。前馈网络的目标是函数逼近。例如对于分类器，$y=f^*(x)%$,映射输入$ｘ$到分类$ｙ$,前馈网络定义个映射$y=f(x:\theta)$,学习参数$\theta$,获得最好的函数逼近。

  这个模型之所以称为前馈，因为通过对参数ｘ进行求值，信息流通过中间计算，最后输出ｙ。该模型没有反馈连接，也没有自反馈。如果前馈神经网络包含反馈连接，我们称为*循环神经网络*（recurrent,neural networks）(参见第十章)

  对于想实践机器学习的人来说，前馈网络非常重要。也是目前很多重要商业应用的基础。例如：卷积网络（一种特殊的前馈网络）用于照片中对象识别，以前馈网络为垫脚石的循环神经网络在自然语言处理上非常有效。

  前馈神经网络之所以称为网络因为由许多不同的函数复合在一起表达。模型一般关联一个*有向循环图*（directed acyclic graph）来描述函数如何复合在一起。例如有三个函数$f^{(1)}$，$f^{(2)}$,$f^{(3)}$连称一个链。形式为$f(x) = $f^{(3)}(f^{(2)}(f^{(1)})$,链式结构是神经网络的最普通结构。一般来说$f^{(1)}$称为第一层网络，$f^{(2)}$称为第二层网络,以此类推。链的长度称为模型的深度。这也是深度学习名字由来。前馈网络的最后一层称为输出层。在神经网络训练过程中，我们尽量$f(x)$匹配$f^*(x)$。训练数据具有噪音，从不同训练点逼近$f^*(x)$,训练例子表明，每个点ｘ都有输出层表达，产生一个值逼近ｙ。其他层不能通过训练数据直接表达。学习算法必须决定如何使用其他层产生期望输出，但是训练数据不能决定每个层如何操作，可以决定如何使用这些层最好的逼近$f^*$,因为训练数据不能显示每层的输出，因此这些层被称为*隐层*

  最后，网络被称为神经网络因为受到神经科学的启发。每个网络的隐层是一个向量值，隐层的维度决定了模型的*宽度*。向量的每个元素的作用类似与神经元。每层是一个向量到想想的函数，也可以看成许多有并行的单元组成，每个单元表示一个向量到标量的的函数。每个单元与神经元非常相似，接受很多其他单元的输入，通过激活函数计算输出。采用向量值表达层的想法来自神经科学，计算表达式的选择函数$f^{(i)}(x)$也来自于生物神经计算科学。但是现代神经网络研究也收益于数据和工程原理，神经网络的目标也不是更好的模拟大脑。前馈网络作为函数逼近机器实现统计概率，大脑一些顿悟等，不仅仅局限于大脑函数模型。

  为了尽快理解前馈网络，我们使用线性模型，考虑如何克服其缺点。线性模型，例如logistic回归和线性回归，使用起来很可靠和有效，甚至好于闭式凸优化。线性模型的缺点是线性函数限制了模型的能力。因此线程模型不能理解两个变量交叉的情况。

  为了拓展线性模型表达非线性函数，对ｘ进行输入变换$\phi(x)$,$\phi$是一个非线性变化。这个小技巧可以获得非线性学习能力。$\phi$提供了一个ｘ的特征描述，或者提供一个ｘ的新表达方式。

  如何选择合适的$\phi$。

  1. 使用一般化$\phi$,例如径向基函数，如果$\phi$的维数足够高，将有足够的空间匹配训练集。
  2. 手动工程化$\phi$，该方法每个任务需要十多年人力努力，参与者从事不同领域，相互没有转换
  3. 深度学习策略是学习$\phi$，定义一个模型$y=f(x;\theta,\omega) = \phi(x:\theta)^\tau\omega$,参数$\theta$用于学习$\phi$

本章讲描述提高模型学习特征的一般原则，
